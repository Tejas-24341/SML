def gradient_descent(X, y, initial_learning_rate=0.01, decay_rate=0.01, n_iterations=1000):
    m = len(y)
    X_b = np.c_[np.ones((m, 1)), X]   
    theta = np.random.randn(2)       

    for iteration in range(n_iterations):
        gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)
        learning_rate = initial_learning_rate / (1 + decay_rate * iteration)
        theta -= learning_rate * gradients
        error = np.mean((X_b.dot(theta) - y) ** 2)

        if error > 0.1:   
            break

    return theta

X = df["age"].values
y = df["shear"].values

theta_gd = gradient_descent(X, y)
print("\nGradient Descent Results:")
print(f"Intercept: {theta_gd[0]}, Slope: {theta_gd[1]}")
